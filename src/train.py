import pyrootutils

root = pyrootutils.setup_root(
    search_from=__file__,
    indicator=[".git", "pyproject.toml"],
    pythonpath=True,
    dotenv=True,
)

# ------------------------------------------------------------------------------------ #
# `pyrootutils.setup_root(...)` is recommended at the top of each start file
# to make the environment more robust and consistent
#
# the line above searches for ".git" or "pyproject.toml" in present and parent dirs
# to determine the project root dir
#
# adds root dir to the PYTHONPATH (if `pythonpath=True`)
# so this file can be run from any place without installing project as a package
#
# sets PROJECT_ROOT environment variable which is used in "configs/paths/default.yaml"
# this makes all paths relative to the project root
#
# additionally loads environment variables from ".env" file (if `dotenv=True`)
#
# you can get away without using `pyrootutils.setup_root(...)` if you:
# 1. move this file to the project root dir or install project as a package
# 2. modify paths in "configs/paths/default.yaml" to not use PROJECT_ROOT
# 3. always run this file from the project root dir
#
# https://github.com/ashleve/pyrootutils
# ------------------------------------------------------------------------------------ #
from typing import Tuple, Dict, List
import mlflow
import lightning.pytorch as L
import hydra
from omegaconf import DictConfig
from lightning.pytorch import Callback, LightningDataModule, LightningModule, Trainer
from lightning.pytorch.loggers import Logger

from lightning.pytorch.loggers.mlflow import MLFlowLogger
from lightning.pytorch.callbacks import LearningRateFinder
import torch
from src import utils
import os
log = utils.get_pylogger(__name__)
from lightning.pytorch.tuner import Tuner

@utils.task_wrapper
def train(cfg: DictConfig) -> Tuple[dict, dict]:
    # set seed for random number generators in pytorch, numpy and python.random
    if cfg.get("seed"):
        L.seed_everything(cfg.seed, workers=True)

    # Override block_size for GPT training while Optuna Hyperparam Optimization
    if bool(cfg['datamodule'].get('block_size', {})):
        cfg.model.block_size     = cfg.datamodule.block_size
        cfg.model.net.block_size = cfg.datamodule.block_size

    log.info(f"Instantiating datamodule <{cfg.datamodule._target_}>")
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)

    log.info(f"Instantiating model <{cfg.model._target_}>")
    model: LightningModule = hydra.utils.instantiate(cfg.model)

    log.info("Instantiating loggers...")
    logger: List[Logger] = utils.instantiate_loggers(cfg.get("logger"))

    log.info("Instantiating callbacks...")
    callbacks: List[Callback] = utils.instantiate_callbacks(cfg.get("callbacks"))

    # Disable Learning Rate finder callbacks for trainer, in case of tuner is enabled.
    if cfg.get("tuner"): 
        callbacks = [callback_ for callback_ in callbacks if not isinstance(callback_, LearningRateFinder)]

    log.info(f"Instantiating trainer <{cfg.trainer._target_}>")
    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, callbacks=callbacks, logger=logger)

    object_dict = {
        "cfg": cfg,
        "datamodule": datamodule,
        "model": model,
        "trainer": trainer,
        "callbacks": callbacks,
        "logger": logger,
    }

    if logger:
        log.info("Logging hyperparameters!")
        utils.log_hyperparameters(object_dict)

    if cfg.get("tuner"):
        log.info("Running LR Finder!")        
        tuner = Tuner(trainer)
        lr_finder = tuner.lr_find(model, datamodule)
        print(f"best initial lr={model.hparams.learning_rate}")
  
        log.info("Running Batch Size Finder!")        
        # Auto-scale batch size by growing it exponentially (default)
        tuner.scale_batch_size(model, datamodule, mode="power")    
        print(f"optimal batch size = {datamodule.hparams.batch_size}")
        
    if cfg.get("train"):
        log.info("Starting training!")
        
        ckpt_save_path = cfg.get('paths', {}).get('ckpt_save_path')
        # checkpoint = ModelCheckpoint(dirpath=ckpt_save_path)

        trainer.fit(model=model, datamodule=datamodule,#callbacks=[checkpoint],
                    ckpt_path=cfg.get("ckpt_path"),
                    )
        
        trainer.save_checkpoint(ckpt_save_path)

    train_metrics = trainer.callback_metrics

    if cfg.get("test"):
        log.info("Starting testing!")
        ckpt_path = trainer.checkpoint_callback.best_model_path
        if ckpt_path == "":
            log.warning(
                "Best ckpt not found! Using current weights for testing...")
            ckpt_path = None
        trainer.test(model=model, datamodule=datamodule, ckpt_path=ckpt_path)
        log.info(f"Best ckpt path: {ckpt_path}")


    for logger_ in logger:
        if isinstance(logger_, MLFlowLogger):
            log.info(f"Started Logging best model")
            ckpt_path = trainer.checkpoint_callback.best_model_path
            if ckpt_path == "":
                log.warning(
                    "Best ckpt not found! Logging current model...")
            else:
                ckpt = torch.load(ckpt_path)
                model.load_state_dict(ckpt["state_dict"])
            os.environ['MLFLOW_RUN_ID'] = logger_.run_id
            os.environ['MLFLOW_EXPERIMENT_ID'] = logger_.experiment_id
            os.environ['MLFLOW_EXPERIMENT_NAME'] = logger_._experiment_name
            os.environ['MLFLOW_TRACKING_URI'] = logger_._tracking_uri
            
            mlflow.pytorch.log_model(model, "model")
            break
    test_metrics = trainer.callback_metrics

    # merge train and test metrics
    metric_dict = {**train_metrics, **test_metrics}

    return metric_dict, object_dict


@hydra.main(version_base="1.3", config_path="../configs", config_name="train.yaml")
def main(cfg: DictConfig):
    # train the model
    metric_dict, _ = train(cfg)

    # this will be used by hydra later for optimization
    # safely retrieve metric value for hydra-based hyperparameter optimization
    metric_value = utils.get_metric_value(
        metric_dict=metric_dict, metric_name=cfg.get("optimized_metric")
    )

    # return optimized metric
    return metric_value


if __name__ == "__main__":
    main()